# posevit/eval_binary.py (TestLabels í´ë” í˜¸í™˜ ë²„ì „)
import os, glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
)

# =====================================
# ë‹¨ì¼ íŒŒì¼ í‰ê°€
# =====================================
def eval_pair(true_csv, pred_csv, out_png=None):
    df_t = pd.read_csv(true_csv)
    df_p = pd.read_csv(pred_csv)

    # ë¼ë²¨ì„ ìˆ«ì(0/1)ë¡œ ë³€í™˜
    y_true = df_t["label"].map({"NoGesture": 0, "Gesture": 1}).to_numpy()
    y_pred = df_p["label"].map({"NoGesture": 0, "Gesture": 1}).to_numpy()

    # ë‘ ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸°
    n = min(len(y_true), len(y_pred))
    y_true, y_pred = y_true[:n], y_pred[:n]

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    P = precision_score(y_true, y_pred, zero_division=0)
    R = recall_score(y_true, y_pred, zero_division=0)
    F = f1_score(y_true, y_pred, zero_division=0)
    A = accuracy_score(y_true, y_pred)

    # Confusion Matrix ì €ì¥
    if out_png:
        cm = confusion_matrix(y_true, y_pred)
        plt.figure(figsize=(4, 3))
        sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=["NoGesture", "Gesture"],
            yticklabels=["NoGesture", "Gesture"]
        )
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.title(os.path.basename(pred_csv))
        plt.tight_layout()
        plt.savefig(out_png)
        plt.close()

    return {"precision": P, "recall": R, "f1": F, "accuracy": A}


# =====================================
# ì „ì²´ ì˜ˆì¸¡ íŒŒì¼ í‰ê°€
# =====================================
def eval_all(pred_dir="outputs/preds", label_dir="data/TestLabels", report_dir="outputs/reports"):
    os.makedirs(report_dir, exist_ok=True)
    pred_files = sorted(glob.glob(os.path.join(pred_dir, "*.mp4_features_predictions.csv")))

    if not pred_files:
        print(f"[âš ï¸ No prediction files found in {pred_dir}]")
        return

    results = []
    print(f"ğŸ” Found {len(pred_files)} prediction files in {pred_dir}\n")

    for pred_csv in pred_files:
        base = os.path.basename(pred_csv).replace(".mp4_features_predictions.csv", "")
        true_csv = os.path.join(label_dir, f"{base}.csv")
        out_png = os.path.join(report_dir, f"{base}_cm.png")

        if not os.path.exists(true_csv):
            print(f"[âš ï¸ Missing ground truth] {true_csv}")
            continue

        try:
            metrics = eval_pair(true_csv, pred_csv, out_png)
            metrics["video"] = base
            results.append(metrics)
            print(f"âœ… Evaluated {base}: F1={metrics['f1']:.4f}, Acc={metrics['accuracy']:.4f}")
        except Exception as e:
            print(f"[âŒ Failed] {base}: {e}")

    if not results:
        print("[âš ï¸ No valid evaluation results found.]")
        return

    # ê²°ê³¼ DataFrame ìƒì„± ë° í‰ê·  ê³„ì‚°
    df = pd.DataFrame(results)
    mean_row = {col: df[col].mean() for col in ["precision", "recall", "f1", "accuracy"]}
    mean_row["video"] = "MEAN"
    df = pd.concat([df, pd.DataFrame([mean_row])], ignore_index=True)

    report_path = os.path.join(report_dir, "eval_summary.csv")
    df.to_csv(report_path, index=False)

    print(f"\nğŸ“Š Summary saved to: {report_path}")
    print(df.tail())

    return df


if __name__ == "__main__":
    eval_all()
