import os
import pandas as pd
import numpy as np
import torch
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from posevit.dataset import PoseSeqDataset
from posevit.model import PoseSeqTransformer as PoseViT
from torch.utils.data import DataLoader
from torch import nn
from tqdm import tqdm
from sklearn.metrics import (
    f1_score, precision_score, recall_score, accuracy_score,
    classification_report  # ‚úÖ Added
)

# =====================
# ÏÑ§Ï†ï
# =====================
INDEX_PATH = "data/index_all.csv"
OUT_DIR = "outputs"
CKPT_PATH = os.path.join(OUT_DIR, "ckpts/posevit_best.pt")
REPORT_PATH = os.path.join(OUT_DIR, "reports/posevit_report.csv")
PLOT_PATH = os.path.join(OUT_DIR, "reports/posevit_comparison.png")
DETAILED_REPORT_PATH = os.path.join(OUT_DIR, "reports/posevit_detailed_report.csv")  # ‚úÖ Added

DEVICE = "cuda:1" if torch.cuda.is_available() else "cpu"
T, S = 32, 16
BATCH_SIZE = 16
EPOCHS = 20
LR = 3e-4
D_IN = 80  # feature Ï∞®Ïõê (Ïòà: MediaPipe upper body Îì±)

# ‚úÖ ÌòÑÏû¨ ÌïôÏäµÏóê ÏÇ¨Ïö© Ï§ëÏù∏ ÎîîÎ∞îÏù¥Ïä§ ÌôïÏù∏
print(f"üöÄ Training on device: {DEVICE} ({'GPU available' if torch.cuda.is_available() else 'CPU only'})")

os.makedirs(os.path.join(OUT_DIR, "ckpts"), exist_ok=True)
os.makedirs(os.path.join(OUT_DIR, "reports"), exist_ok=True)

# =====================
# 1Ô∏è‚É£ Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ¶¨
# =====================
all_index = pd.read_csv(INDEX_PATH)
train_df, temp_df = train_test_split(all_index, test_size=0.3, random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

print(f"‚úÖ Train: {len(train_df)} clips, Val: {len(val_df)}, Test: {len(test_df)}")

train_df.to_csv("data/index_train.csv", index=False)
val_df.to_csv("data/index_val.csv", index=False)
test_df.to_csv("data/index_test.csv", index=False)

# =====================
# 2Ô∏è‚É£ Dataset & Dataloader
# =====================
train_ds = PoseSeqDataset("data/index_train.csv", T=T, S=S)
val_ds = PoseSeqDataset("data/index_val.csv", T=T, S=S)
test_ds = PoseSeqDataset("data/index_test.csv", T=T, S=S)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)
test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)

# =====================
# 3Ô∏è‚É£ Î™®Îç∏ Ï†ïÏùò
# =====================
model = PoseViT(d_in=D_IN, d_model=128, nhead=4, num_layers=2, num_classes=2).to(DEVICE)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LR)

# =====================
# 4Ô∏è‚É£ ÌïôÏäµ Î£®ÌîÑ
# =====================
best_f1 = 0
for epoch in range(1, EPOCHS + 1):
    model.train()
    for Xb, yb in tqdm(train_loader, desc=f"Train {epoch}"):
        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
        optimizer.zero_grad()
        logits = model(Xb)
        loss = criterion(logits.view(-1, 2), yb.view(-1))
        loss.backward()
        optimizer.step()

    # === Validation ===
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for Xb, yb in val_loader:
            Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
            pred = model(Xb).argmax(dim=-1)
            y_true.extend(yb.cpu().numpy().flatten())
            y_pred.extend(pred.cpu().numpy().flatten())

    f1 = f1_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    acc = accuracy_score(y_true, y_pred)

    print(f"[Val] ep{epoch} F1={f1:.4f} P={prec:.4f} R={rec:.4f} Acc={acc:.4f}")

    # ÏµúÍ≥† F1 Î™®Îç∏ Ï†ÄÏû• (cfg Ìè¨Ìï®)
    if f1 > best_f1:
        best_f1 = f1
        ckpt = {
            "cfg": {
                "d_in": D_IN,
                "d_model": 128,
                "nhead": 4,
                "num_layers": 2,
                "num_classes": 2,
                "T": T,
                "S": S,
                "lr": LR,
                "epochs": EPOCHS
            },
            "model": model.state_dict()
        }
        torch.save(ckpt, CKPT_PATH)
        print(f"‚úì Saved best model to {CKPT_PATH}")

# =====================
# 5Ô∏è‚É£ ÌÖåÏä§Ìä∏ ÌèâÍ∞Ä
# =====================
ckpt = torch.load(CKPT_PATH, map_location=DEVICE)
model.load_state_dict(ckpt["model"])
model.eval()

def evaluate(loader):
    y_true, y_pred = [], []
    with torch.no_grad():
        for Xb, yb in loader:
            Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)
            pred = model(Xb).argmax(dim=-1)
            y_true.extend(yb.cpu().numpy().flatten())
            y_pred.extend(pred.cpu().numpy().flatten())
    return y_true, y_pred

# ‚úÖ Validation/Test Íµ¨Î∂Ñ
y_true_val, y_pred_val = evaluate(val_loader)
y_true_test, y_pred_test = evaluate(test_loader)

val_metrics = {
    "precision": precision_score(y_true_val, y_pred_val),
    "recall": recall_score(y_true_val, y_pred_val),
    "f1": f1_score(y_true_val, y_pred_val),
    "accuracy": accuracy_score(y_true_val, y_pred_val)
}

test_metrics = {
    "precision": precision_score(y_true_test, y_pred_test),
    "recall": recall_score(y_true_test, y_pred_test),
    "f1": f1_score(y_true_test, y_pred_test),
    "accuracy": accuracy_score(y_true_test, y_pred_test)
}

# =====================
# 6Ô∏è‚É£ Í≤∞Í≥º Ï†ÄÏû•
# =====================
df = pd.DataFrame([
    {"split": "Validation", **val_metrics},
    {"split": "Test", **test_metrics}
])
df.to_csv(REPORT_PATH, index=False)
print(f"üìä Report saved to {REPORT_PATH}")

# ‚úÖ Added: classification_report
report = classification_report(
    y_true_test,
    y_pred_test,
    target_names=["NoGesture", "Gesture"],
    output_dict=True
)
report_df = pd.DataFrame(report).transpose()
report_df["accuracy"] = accuracy_score(y_true_test, y_pred_test)
report_df.to_csv(DETAILED_REPORT_PATH)
print(f"üìÑ Detailed classification report saved to {DETAILED_REPORT_PATH}")
print(report_df)

# =====================
# 7Ô∏è‚É£ Í∑∏ÎûòÌîÑ ÏãúÍ∞ÅÌôî
# =====================
plt.figure(figsize=(6, 4))
df_melt = df.melt(id_vars="split", var_name="metric", value_name="score")
for metric in ["precision", "recall", "f1", "accuracy"]:
    plt.bar(df_melt[df_melt["metric"] == metric]["split"],
            df_melt[df_melt["metric"] == metric]["score"], alpha=0.7, label=metric)
plt.legend()
plt.title("Validation vs Test Performance")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.tight_layout()
plt.savefig(PLOT_PATH)
print(f"üìà Plot saved to {PLOT_PATH}")
