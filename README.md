# 📖 Noname
👨‍🎓 Goethe-Universität Frankfurt am Main \
🏛️ Fachbereich 12 Institut für Informatik \
📫 E-mail: S6010479@stud.uni-frankfurt.de \

## Title: Noname

Betreuer: Prof. Dr. A*** M***, Dr. A*** L***, Dr. A*** H***

###### Envisionhgdetector: https://envisionbox.org/embedded_UsingEnvisionHGdetector_package.html \
###### 💿 Dataset:
1. **TTLab Goethe-Universität**
2. **University of Edinburgh, Centre for Language Evolution link:** https://datashare.ed.ac.uk/handle/10283/3191
3. **IFADV link:** https://www.fon.hum.uva.nl/IFA-SpokenLanguageCorpora/IFADVcorpus/


###### 🔗 Github: https://github.com/WimPouw/envisionhgdetector

###### 📚 Pythonlibrary: https://pypi.org/project/envisionhgdetector/ 


### Schritt:
1. Mit einem Tool für Binäre Klassifikation erkennen Gesten bzw. BIO-Label oder Gesten, Non-Gesten
2. Datensatz: R-Daten aus Va.Si.Li-Lab verwendet werden. Dazu gehören die Multiperspektiven-Videos 
3. Tool auswählen : Envisionhgdetector (Empfehlung von Dr. Henlein)
4. Ausführen den Envisionhgdetector mit Datensatz, ob er Gesten gut erkennt.
5. noch offen

***

### Aktueller Stand
1. envisionhgdetector testen
  - [x] TTLab Dataset
  - [x] University of Edinburgh Dataset
2. Evaluation von Envisiohgdetector
  - [ ] Labeling
  - [ ] F1-Score, accuracy,   
