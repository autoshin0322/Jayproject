# ğŸ“– Noname
ğŸ‘¨â€ğŸ“ Goethe-UniversitÃ¤t Frankfurt am Main \
ğŸ›ï¸ Fachbereich 12 Institut fÃ¼r Informatik \
ğŸ“« E-mail: S6010479@stud.uni-frankfurt.de \

## Title: Noname

Betreuer: Prof. Dr. A*** M***, Dr. A*** L***, Dr. A*** H***

###### Envisionhgdetector: https://envisionbox.org/embedded_UsingEnvisionHGdetector_package.html \
###### ğŸ’¿ Dataset:
1. **TTLab Goethe-UniversitÃ¤t**
2. **University of Edinburgh, Centre for Language Evolution link:** https://datashare.ed.ac.uk/handle/10283/3191
3. **IFADV link:** https://www.fon.hum.uva.nl/IFA-SpokenLanguageCorpora/IFADVcorpus/


###### ğŸ”— Github: https://github.com/WimPouw/envisionhgdetector

###### ğŸ“š Pythonlibrary: https://pypi.org/project/envisionhgdetector/ 


### Schritt:
1. Mit einem Tool fÃ¼r BinÃ¤re Klassifikation erkennen Gesten bzw. BIO-Label oder Gesten, Non-Gesten
2. Datensatz: R-Daten aus Va.Si.Li-Lab verwendet werden. Dazu gehÃ¶ren die Multiperspektiven-Videos 
3. Tool auswÃ¤hlen : Envisionhgdetector (Empfehlung von Dr. Henlein)
4. AusfÃ¼hren den Envisionhgdetector mit Datensatz, ob er Gesten gut erkennt.
5. noch offen

***

### Aktueller Stand
1. envisionhgdetector testen
  - [x] TTLab Dataset
  - [x] University of Edinburgh Dataset
2. Evaluation von Envisiohgdetector
  - [ ] Labeling
  - [ ] F1-Score, accuracy,   
