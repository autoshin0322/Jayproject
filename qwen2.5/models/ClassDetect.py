import os
import csv
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

torch.cuda.empty_cache()
torch.cuda.ipc_collect()

# -----------------------------------------------------------
# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
# -----------------------------------------------------------
MODEL_NAME = "Qwen/Qwen2.5-VL-7B-Instruct"
device = "cuda:0" if torch.cuda.is_available() else "cpu"

model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    MODEL_NAME, torch_dtype="auto", device_map="cuda:0"
)
processor = AutoProcessor.from_pretrained(MODEL_NAME)

# -----------------------------------------------------------
# 2. ì…ë ¥ / ì¶œë ¥ ê²½ë¡œ
# -----------------------------------------------------------
INPUT_DIR = "video42"             # ëª¨ë“  .mp4 íŒŒì¼ì´ ë“¤ì–´ìˆëŠ” í´ë”
OUTPUT_CSV = "Video42Output.csv"      # ê²°ê³¼ ì €ì¥ íŒŒì¼

# -----------------------------------------------------------
# 3. CSV í—¤ë” ìƒì„±
# -----------------------------------------------------------
with open(OUTPUT_CSV, mode="w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["index", "filename", "label"])

# -----------------------------------------------------------
# 4. ì˜ìƒ íŒŒì¼ ë°˜ë³µ ì²˜ë¦¬ (ìµœëŒ€ 10ê°œ)
# -----------------------------------------------------------
#video_files = [f for f in sorted(os.listdir(INPUT_DIR)) if f.lower().endswith(".mp4")]
#video_files = video_files[:10]  # âœ… ì•ì˜ 10ê°œ íŒŒì¼ë§Œ ì„ íƒ

#for file_name in video_files:
#    video_path = os.path.join(INPUT_DIR, file_name)
#    print(f"ğŸ¥ Processing: {file_name}")

for file_name in sorted(os.listdir(INPUT_DIR)):
    if not file_name.lower().endswith(".mp4"):
        continue

    video_path = os.path.join(INPUT_DIR, file_name)
    print(f"ğŸ¥ Processing: {file_name}")

    # ë©”ì‹œì§€ êµ¬ì„± (ì›ë³¸ êµ¬ì¡° ìœ ì§€)
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "video",
                    "video": video_path,
                    "max_pixels": 360 * 420,
                    "fps": 32.0,
                },
                {
                    "type": "text",
                    "text": ("You are an expert in hand gesture recognition and classification based on visual input. Analyze the given video (including audio and transcript if available), and classify the primary hand gesture performed in the video into one of the following eight categories: representing, molding, indexing, drawing, other, beat, emblematic, or acting. Each label is defined as follows: representing: gestures that describe an object, shape, or scene / molding: gestures that simulate shaping or transforming objects with the hands / indexing: gestures that point to a direction, object, or place / drawing: gestures that mimic drawing in the air / other: gestures that donâ€™t clearly fit into any of the categories / beat: rhythmic gestures that follow the flow of speech without semantic meaning / emblematic: culturally defined gestures with fixed meanings (e.g., thumbs up, peace sign) / acting: gestures that mime an action or movement. Make your decision based on the visible hand movement; use speech content only to support your interpretation when necessary. Be precise and avoid ambiguity. Output must follow this rule: if I request explanation, output the label followed by a 1â€“2 sentence reason explaining your decision; if I request no explanation, output only the classification label. Classify this gesture with no explanation. Take a deep breath and letâ€™s work this out in a step-by-step way to make sure we get the right answer."
                    ),
                },
            ],
        }
    ]

    # ì¸í¼ëŸ°ìŠ¤ ì…ë ¥ ì¤€ë¹„
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    ).to(device)

    # ëª¨ë¸ ì‹¤í–‰
    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=128)

    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0].strip()

    print(f"ğŸ§  Model Output: {output_text}")

    # CSVì— ì €ì¥
    with open(OUTPUT_CSV, mode="a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([file_name, output_text])

print("\nâœ… 10ê°œ ì˜ìƒ ì²˜ë¦¬ ì™„ë£Œ!")
print(f"ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {OUTPUT_CSV}")
