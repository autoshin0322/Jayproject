import os
import csv
import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# -----------------------------------------------------------
# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
# -----------------------------------------------------------
MODEL_NAME = "Qwen/Qwen2.5-VL-7B-Instruct"
device = "cuda" if torch.cuda.is_available() else "cpu"

model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    MODEL_NAME, torch_dtype="auto", device_map="auto"
)
processor = AutoProcessor.from_pretrained(MODEL_NAME)

# -----------------------------------------------------------
# 2. ì…ë ¥ / ì¶œë ¥ ê²½ë¡œ
# -----------------------------------------------------------
INPUT_DIR = "videos_to_label"             # ëª¨ë“  .mp4 íŒŒì¼ì´ ë“¤ì–´ìˆëŠ” í´ë”
OUTPUT_CSV = "results_classification(explanation).csv"      # ê²°ê³¼ ì €ì¥ íŒŒì¼

# -----------------------------------------------------------
# 3. CSV í—¤ë” ìƒì„±
# -----------------------------------------------------------
with open(OUTPUT_CSV, mode="w", newline="", encoding="utf-8") as f:
    writer = csv.writer(f)
    writer.writerow(["filename", "label"])

# -----------------------------------------------------------
# 4. ì˜ìƒ íŒŒì¼ ë°˜ë³µ ì²˜ë¦¬ (ìµœëŒ€ 10ê°œ)
# -----------------------------------------------------------
#video_files = [f for f in sorted(os.listdir(INPUT_DIR)) if f.lower().endswith(".mp4")]
#video_files = video_files[:10]  # âœ… ì•ì˜ 10ê°œ íŒŒì¼ë§Œ ì„ íƒ

#for file_name in video_files:
#    video_path = os.path.join(INPUT_DIR, file_name)
#    print(f"ğŸ¥ Processing: {file_name}")

for file_name in sorted(os.listdir(INPUT_DIR)):
    if not file_name.lower().endswith(".mp4"):
        continue

    video_path = os.path.join(INPUT_DIR, file_name)
    print(f"ğŸ¥ Processing: {file_name}")

    # ë©”ì‹œì§€ êµ¬ì„± (ì›ë³¸ êµ¬ì¡° ìœ ì§€)
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "video",
                    "video": video_path,
                    "max_pixels": 360 * 420,
                    "fps": 32.0,
                },
                {
                    "type": "text",
                    "text": ("You are a gesture classification assistant. Your task is to analyze the hand gesture shown in the given video. Choose only one label from the following list: [representing, molding, acting, indexing, other, beat, drawing, emblematic]. Do not always choose the same label. Base your answer on the motion, hand shape, and context shown in the video."
                    ),
                },
            ],
        }
    ]

    # ì¸í¼ëŸ°ìŠ¤ ì…ë ¥ ì¤€ë¹„
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    ).to(device)

    # ëª¨ë¸ ì‹¤í–‰
    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=128)

    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0].strip()

    print(f"ğŸ§  Model Output: {output_text}")

    # CSVì— ì €ì¥
    with open(OUTPUT_CSV, mode="a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([file_name, output_text])

print("\nâœ… 10ê°œ ì˜ìƒ ì²˜ë¦¬ ì™„ë£Œ!")
print(f"ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {OUTPUT_CSV}")
